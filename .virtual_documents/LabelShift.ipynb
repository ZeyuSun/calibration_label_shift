import numpy as np
import matplotlib.pyplot as plt


def sampling(nc,n,m,Py,Qy):
    mu = np.arange(1,nc+1)
    Py_hat = np.random.multinomial(n, Py)
    Qy_hat = np.random.multinomial(m, Qy)
    Dset_p = np.random.normal(0,sigma,n)
    Dset_q = np.random.normal(0,sigma,m)

    DpX = np.zeros((n,1))
    DpY = np.zeros((n))
    DpY_vec = np.zeros((n,nc))
    j = 0
    for i in range(nc):
        DpX[j:j+Py_hat[i],0] = Dset_p[j:j+Py_hat[i]] + mu[i]
        DpY[j:j+Py_hat[i]] = DpY[j:j+Py_hat[i]] + i
        DpY_vec[j:j+Py_hat[i],i] = 1
        j = j+Py_hat[i]

    DqX = np.zeros((n,1))
    DqY = np.zeros((n))
    DqY_vec = np.zeros((n,nc))
    j = 0
    for i in range(nc):
        DqX[j:j+Qy_hat[i],0] = Dset_q[j:j+Qy_hat[i]] + mu[i]
        DqY[j:j+Qy_hat[i]] = DqY[j:j+Qy_hat[i]] + i
        DqY_vec[j:j+Qy_hat[i],i] = 1
        j = j+Qy_hat[i]
    return DpX, DpY, DqX, DqY, DpY_vec, DqY_vec


nc = 20
epoch = 2
n = 1000 * (epoch+1)
m = 1000 * (epoch+1)

#shift
Py = np.zeros(nc) ; Py[0:int(nc):2] = 1/nc ; Py[1:int(nc):2] = 3/nc  ; Py = Py/np.sum(np.abs(Py))
Qy = np.zeros(nc) ; Qy[0:int(nc):2] = 3/nc  ; Qy[1:int(nc):2] = 1/nc ; Qy = Qy/np.sum(np.abs(Qy))
iw = Qy/Py
#samples
sigma = 0.3

DpX, DpY, DqX, DqY, DpY_vec, DqY_vec=sampling(nc,n,m,Py,Qy)


from sklearn.neural_network import MLPClassifier


import plotly.express as px
px.histogram(x=DpX.flatten(), color=DpY, nbins=600, barmode='overlay', opacity=0.5)


clf = MLPClassifier(
    solver='sgd',
    alpha=1e-3,
    learning_rate = 'adaptive',
    learning_rate_init= 1e-3,
    max_iter=5000,
    activation='relu',
    hidden_layer_sizes=(50, 200, 500, 200, 50),
)


clf.fit(DpX, DpY)


plt.plot(clf.loss_curve_)


clf.score(DpX, DpY)


clf.score(DqX, DqY)


clf = MLPClassifier(
    solver='adam',
    alpha=1e-3,
    # learning_rate = 'adaptive',
    learning_rate_init= 1e-3,
    max_iter=1000,
    activation='relu',
    hidden_layer_sizes=(10, 10, 10),
)


clf.fit(DpX, DpY)


plt.plot(clf.loss_curve_)


clf.score(DpX, DpY)


clf.score(DqX, DqY)


from collections import Counter
from sklearn.metrics import confusion_matrix


def get_frequency(y):
    cnt = Counter(y)
    cnt_list = [cnt[k] for k in range(len(cnt))]
    freq = np.array(cnt_list) / len(y)
    return freq


def fit_BBSE(clf, Xs, ys, Xt, soft=False):
    #  Black Box Shift Estimation (BBSE; Lipton'18)
    if not soft:
        yp = clf.predict(Xs)
        C = confusion_matrix(ys, yp).T
    else:
        yp = clf.predict_proba(Xs)
        K = len(np.unique(ys))
        C = [np.sum(yp[ys==k]) for k in range(K)]
        C = np.array(C).T / len(ys)
    q = get_frequency(clf.predict(Xt))
    w = np.linalg.solve(C, q)
    if np.any(w <= 0):
        print(f'Warning: nonpositive w: {w}')
        w = np.maximum(w, 0)
    return w


def fit_RLLS(clf, Xs, ys, Xt, lam=1):
    # Regularized Learning under Label Shift (RLLS; Azizzadenesheli'22)
    yp = clf.predict(Xs)
    C = confusion_matrix(ys, yp).T
    q = get_frequency(clf.predict(Xt))
    b = q - np.sum(C, axis=1)
    theta = np.linalg.solve(C, b)
    w = theta * lam + 1
    return w


def MLLS_EM(posterior_source,
            prior_source,
            max_iter=100):
    # Saerens'02
    # posterior_source: source posterior on the test data.
    # prior_source:
    w = np.ones(posterior_source.shape[1])
    history = []
    for i in range(max_iter):
        ps_w = posterior_source * w
        posterior_target = ps_w / np.sum(ps_w, axis=1, keepdims=True)
        w = np.mean(posterior_target, axis=0) / pi
        objective = np.mean(np.log(posterior_target @ w)) # Eq 3, Garg'20
        history.append(objective)
    return w, posterior_target, history


def fit_MLLS_v1(clf, Xs, ys, Xt,
                max_iter=100):
    # Saerens'02
    eta_s = clf.predict_proba(Xt) # source posterior
    pi_s = get_frequency(ys)
    w, eta_t, history = MLLS_EM(eta_s, pi_s, max_iter=max_iter)
    return w, eta_t, history


def fit_MLLS_v2(clf, Xs, ys, Xt,
                max_iter=100,
                return_posterior=False):
    # Alexandari'20
    eta_s = clf.predict_proba(Xt) # source posterior
    pi_s = np.mean(eta_s, axis=0)
    w, eta_t, history = MLLS_EM(eta_s, pi_s, max_iter=max_iter)
    return w, eta_t, history
